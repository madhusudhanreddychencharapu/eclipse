<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Manage OpenShift Streams for Apache Kafka with AKHQ</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/26/manage-openshift-streams-apache-kafka-akhq" /><author><name>Duncan Doyle</name></author><id>f3e9a651-4395-4243-a975-06fa08224334</id><updated>2022-12-26T07:00:00Z</updated><published>2022-12-26T07:00:00Z</published><summary type="html">&lt;p&gt;At Red Hat, we are often asked what consoles and graphical user interfaces (GUIs) can be used with our Kafka products, &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;Red Hat AMQ Streams&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. Because our products are fully based on the upstream &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka project&lt;/a&gt;, most available consoles and GUIs designed to work with Kafka also work with our Kafka products. This article illustrates the ease of integration through a look at &lt;a data-href="https://akhq.io" href="https://akhq.io/"&gt;AKHQ&lt;/a&gt;, an open source GUI for Apache Kafka.&lt;/p&gt; &lt;h2&gt;Using Apache Kafka&lt;/h2&gt; &lt;p&gt;Apache Kafka has quickly become the leading event-streaming platform, enabling organizations to unlock and use their data in new and innovative ways. With Apache Kafka, companies can bring more value to their products by processing real-time events more quickly and accurately.&lt;/p&gt; &lt;p&gt;At a high level, Apache Kafka's architecture is quite simple. It's based on a few concepts such as brokers, topics, partitions, producers, and consumers. However—as with any system—when you deploy, operate, manage, and monitor a production Kafka cluster, things can quickly become complex. To use and manage Kafka clusters in both development and production environments, there are numerous tools on the market, both commercial and open source. These tools range from scripts, GUIs, and powerful command-line interfaces (CLIs) to full monitoring, management, and governance platforms. Each type of tool offers value in specific parts of the software development cycle.&lt;/p&gt; &lt;p&gt;This article shows how to connect AKHQ to a Kafka instance in Red Hat OpenShift Streams for Apache Kafka, a managed cloud service. Using our no-cost, 48-hour trial of OpenShift Streams, you can follow along with the steps. By the end of the article, you will be able to use AKHQ to manage your Kafka instance.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow the instructions in this article, you'll need the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;A Kafka instance in Red Hat OpenShift Streams for Apache Kafka (either a trial instance or full instance). A 48-hour trial instance is available at no cost. Go to the &lt;a data-href="https://console.redhat.com/application-services/streams" href="https://console.redhat.com/application-services/streams"&gt;Red Hat console&lt;/a&gt;, log in with your Red Hat account (or create one), and create a trial instance. You can learn how to create your first Kafka instance in &lt;a data-href="https://console.redhat.com/application-services/learning-resources?quickstart=getting-started" href="https://console.redhat.com/application-services/learning-resources?quickstart=getting-started"&gt;this quick start guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;code&gt;rhoas&lt;/code&gt; CLI. This is a powerful command-line interface for managing various Red Hat OpenShift Application Services resources, such as Kafka instances, Service Registry instances, service accounts, and more. Download the CLI at this &lt;a href="https://github.com/redhat-developer/app-services-cli"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A container runtime such as Podman or Docker to run the AKHQ container image used in this article.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The sections that follow show how to generate connection information for a Kafka instance in OpenShift Streams and then use this information to connect AKHQ to the Kafka instance.&lt;/p&gt; &lt;h2 id="_creating_a_service_account"&gt;Creating a service account&lt;/h2&gt; &lt;p&gt;For AKHQ to connect to your Kafka instance, it needs credentials to authenticate with OpenShift Streams for Apache Kafka. In the OpenShift Application Services world, this means that you must create a service account. You can do this using the web console, the CLI, or a REST API call. The following steps show how to do so using the web console:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Navigate to &lt;a data-href="https://console.redhat.com/application-services/service-accounts" href="https://console.redhat.com/application-services/service-accounts"&gt;https://console.redhat.com/application-services/service-accounts&lt;/a&gt; and click &lt;strong&gt;Create service account&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enter a name for the service account. In this case, let's call it &lt;code&gt;akhq-sa&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Copy the generated &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client secret&lt;/strong&gt; values to a secure location. You'll specify these credentials when configuring a connection to the Kafka instance.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;After you save the generated credentials to a secure location, select the confirmation check box and click &lt;strong&gt;Close&lt;/strong&gt;, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/service-account.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/service-account.png?itok=2D_nlEro" width="600" height="286" alt="A dialog box shows that security credentials were generated." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A dialog box shows that security credentials were generated. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Configuring Kafka ACLs&lt;/h2&gt; &lt;p&gt;Before AKHQ can use the service account to connect to your Kafka instance in OpenShift Streams, you need to configure authorization of the account using Kafka &lt;em&gt;Access Control List&lt;/em&gt; (ACL) permissions. At a minimum, you need to give the service account permissions to create and delete topics and produce and consume messages.&lt;/p&gt; &lt;p&gt;To configure ACL permissions for the service account, this article uses the &lt;code&gt;rhoas&lt;/code&gt; CLI. You can also use the web console or the REST API to configure the permissions.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In this article, we show how to use AKHQ to manage topics and consumer groups. Therefore, we set only the ACLs required to manage those resources. If you want to manage other Kafka resources—for example, the Kafka broker configuration—you might need to configure additional ACL permissions for your service account. However, always be sure to set &lt;em&gt;only&lt;/em&gt; the permissions needed for your use case. Having minimal permissions limits the attack surface of your Kafka cluster, improving security.&lt;/p&gt; &lt;p&gt;The following steps show how to set the required ACL permissions for your service account.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Log in to the &lt;code&gt;rhoas&lt;/code&gt; CLI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;rhoas login&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A login flow opens in your web browser.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Log in with the Red Hat account that you used to create your OpenShift Streams instance.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Specify the Kafka instance in OpenShift Streams that you would like to use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;rhoas kafka use&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;rhoas kafka use&lt;/code&gt; command sets the OpenShift Streams instance in the context of your CLI, meaning that subsequent CLI operations (for example, setting ACLs), are performed against this Kafka instance.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Set the required Kafka ACL permissions, as shown in the following example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export CLIENT_ID=&lt;your-service-account-client-id&gt; rhoas kafka acl grant-access --producer --consumer --service-account $CLIENT_ID --topic "*" --group "*" -y &amp;&amp; \ rhoas kafka acl create --operation delete --permission allow --topic "*" --service-account $CLIENT_ID -y &amp;&amp; \ rhoas kafka acl create --operation alter-configs --permission allow --topic "*" --service-account $CLIENT_ID -y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The example invokes the &lt;code&gt;grant-access&lt;/code&gt; subcommand to set ACL permissions to produce and consume messages in any topic in the Kafka instance. To fully manage topics from AKHQ, subsequent commands allow the &lt;code&gt;delete&lt;/code&gt; and &lt;code&gt;alter-configs&lt;/code&gt; operations on any topic. Output from the commands is shown in Figure 2, indicating that the desired permissions and ACLs were created.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhoas-set-acls.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhoas-set-acls.png?itok=dL0CrCHs" width="600" height="309" alt="Output from the ACL commands show the permissions and ACLs generated." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Output from the ACL commands show the permissions and ACLs generated. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Output from the ACL commands shows the permissions and ACLs generated.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This example uses the * wildcard character to set permissions for all topics in the Kafka cluster. You can limit access by setting the permissions for a specific topic name or for a set of topics using a prefix.&lt;/p&gt; &lt;h2 id="_connecting_akhq_to_openshift_streams_for_apache_kafka"&gt;Connecting AKHQ to OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;With your ACL permissions in place, you can now configure AKHQ to connect to your Kafka instance in OpenShift Streams. There are multiple ways to run AKHQ. The &lt;a data-href="https://akhq.io/docs/installation.html" href="https://akhq.io/docs/installation.html"&gt;AKQH installation documentation&lt;/a&gt; describes the various options in detail.&lt;/p&gt; &lt;p&gt;This article runs AKHQ in a container. This means that you need a container runtime such as Podman or Docker. We show how to run the container using Docker Compose, but you can also use Podman Compose with the same compose file.&lt;/p&gt; &lt;p&gt;The AKHQ configuration in this example is a very basic configuration that connects to an OpenShift Streams Kafka instance using Simple Authentication Security Layer (SASL) OAuthBearer authentication. The configuration uses the client ID and client secret values of your service account and an OAuth token endpoint URL, which is required for authentication with &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on (SSO) technology&lt;/a&gt;. The example configuration looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;akhq: # list of kafka cluster available for akhq connections: openshift-streams-kafka: properties: bootstrap.servers: "${BOOTSTRAP_SERVER}" security.protocol: SASL_SSL sasl.mechanism: OAUTHBEARER sasl.jaas.config: &gt; org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required oauth.client.id="${CLIENT_ID}" oauth.client.secret="${CLIENT_SECRET}" oauth.token.endpoint.uri="${OAUTH_TOKEN_ENDPOINT_URI}" ; sasl.login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll notice that the configuration also uses the &lt;code&gt;OauthLoginCallbackHandler&lt;/code&gt; class from the &lt;a data-href="https://strimzi.io/" href="https://strimzi.io/"&gt;Strimzi project&lt;/a&gt;. This callback handler class is packaged by default with AKHQ, enabling you to use OAuthBearer authentication against OpenShift Streams.&lt;/p&gt; &lt;p&gt;The configuration shown is included in the &lt;code&gt;docker-compose.yml&lt;/code&gt; file that we'll use to run our containerized AKHQ instance. You can find the &lt;code&gt;docker-compose.yml&lt;/code&gt; file in &lt;a data-href="https://github.com/DuncanDoyle/rhosak-akhq-blog" href="https://github.com/DuncanDoyle/rhosak-akhq-blog"&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following steps show to use Docker Compose to connect AKHQ to your Kafka instance in OpenShift Streams:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Clone the GitHub repository that has the example &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/DuncanDoyle/rhosak-akhq-blog.git cd rhosak-akhq-blog&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Set environment variables that define connection information for your Kafka instance.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export CLIENT_ID=&lt;your-service-account-client-id&gt; export CLIENT_SECRET=&lt;your-service-account-client-secret&gt; export BOOTSTRAP_SERVER=&lt;your-kafka-bootstrap-server-url-and-port&gt; export OAUTH_TOKEN_ENDPOINT_URI=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These environment variables specify the client ID and client secret values of your service account, the OAuth token endpoint URL used for authentication with Red Hat's SSO service, and the bootstrap server URL and port of your Kafka instance.&lt;/p&gt; &lt;p&gt;You can get the bootstrap server information for your Kafka instance in the OpenShift Streams web console or by using the &lt;code&gt;rhoas kafka describe&lt;/code&gt; CLI command. You can also get the OAuth token endpoint URL in the web console. However, because this URL is a static value in OpenShift Streams, you can simply set it to &lt;code&gt;&lt;a data-href="https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token" href="https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token"&gt;https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/a&gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Figure 3 shows how to get the bootstrap server and token URL in the web console.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhosak-bootstrap-server.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhosak-bootstrap-server.png?itok=okypM3oh" width="600" height="286" alt="Choose your Kafka instance to find the bootstrap server and token URL." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Choose your Kafka instance to find the bootstrap server and token URL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Start AKHQ using &lt;code&gt;docker-compose&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;docker-compose up&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The AKHQ management console becomes available at &lt;code&gt;http://localhost:8080&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;If you've configured everything correctly, when you hover your mouse over the datastore icon, you should see an &lt;code&gt;openshift-streams-kafka&lt;/code&gt; connection, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhosak-akhq-empty.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhosak-akhq-empty.png?itok=FU1QfFez" width="600" height="314" alt="A Kafka connection is available, with no topics." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: A Kafka connection is available, with no topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2 id="_managing_openshift_streams"&gt;Managing OpenShift Streams&lt;/h2&gt; &lt;p&gt;With the AKHQ management console connected, you can now use the console to interact with your Kafka instance in OpenShift Streams.&lt;/p&gt; &lt;p&gt;The following steps show how to create a topic, produce some data, and inspect the data that you've produced to the topic. You can use the OpenShift Streams web console and &lt;code&gt;rhoas&lt;/code&gt; CLI to do these things, but remember that the point of this example is to show how you can use the AKHQ console!&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a topic. In the lower-right corner of the AKHQ console, click &lt;strong&gt;Create a topic&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;On the topic creation page, name the topic &lt;code&gt;my-topic&lt;/code&gt;, keep the default values for all the other options, and click &lt;strong&gt;Create&lt;/strong&gt;, as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq-create-topic.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/akhq-create-topic.png?itok=vRoCjxoN" width="600" height="315" alt="The "Create a topic" page lets you assign a name and other attributes." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The "Create a topic" page lets you assign a name and other attributes. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If you have set ACL permissions on the service account correctly, you see the topic that you just created. You can also see the same topic in the OpenShift Streams web console. Figure 6 shows the same topic as it appears in both consoles.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq-topic-created.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/akhq-topic-created.png?itok=8123dqOo" width="600" height="285" alt="Topic in the AKHQ console and the OpenShift Streams console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Topic in the AKHQ console and the OpenShift Streams console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;To perform tasks such as inspecting and producing messages, viewing consumer groups, inspecting consumer lag, and so on, click your new topic in the AKHQ console. Figure 7 shows a message in JSON format that has come into the topic, and Figure 8 shows statistics about the topic. &lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/akhq-messages.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/akhq-messages.png?itok=DQehnZMn" width="600" height="314" alt="A message appears in JSON format in the topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: A message appears in JSON format in the topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig8_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig8_1.png?itok=_YUwWVDL" width="600" height="313" alt="Screenshot showing statistics about your Kafka topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: Statistics about your Kafka topic. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2 id="_conclusion"&gt;Red Hat simplifies access to open source tools&lt;/h2&gt; &lt;p&gt;This article has shown how to manage and monitor a Kafka instance in Red Hat OpenShift Streams for Apache Kafka using AKHQ. For more information about AKHQ, see the &lt;a data-href="https://akhq.io/docs/" href="https://akhq.io/docs/"&gt;AKHQ documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;On a larger scale, the article illustrates the ability to use popular tools from the open source Kafka ecosystem with Red Hat's managed cloud services. This openness gives you the flexibility you need when building enterprise-scale systems based on open source technologies. The use of open standards and non-proprietary APIs and protocols in Red Hat service offerings enables seamless integration with various technologies.&lt;/p&gt; &lt;p&gt;If you haven't yet done so, please visit the &lt;a data-href="https://console.redhat.com/" href="https://console.redhat.com/"&gt;Red Hat Hybrid Cloud Console&lt;/a&gt; for more information about OpenShift Streams for Apache Kafka, as well as our other service offerings. OpenShift Streams for Apache Kafka provides a 48-hour trial version of our product at no cost. We encourage you to give it a spin.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/26/manage-openshift-streams-apache-kafka-akhq" title="Manage OpenShift Streams for Apache Kafka with AKHQ"&gt;Manage OpenShift Streams for Apache Kafka with AKHQ&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Duncan Doyle</dc:creator><dc:date>2022-12-26T07:00:00Z</dc:date></entry><entry><title type="html">Announcing Dashbuilder Quarkus Extension</title><link rel="alternate" href="https://blog.kie.org/2022/12/announcing-dashbuilder-quarkus-extension.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/12/announcing-dashbuilder-quarkus-extension.html</id><updated>2022-12-23T16:53:23Z</updated><content type="html">We are glad to announce that we released the ! You can now render dashboards directly in your Quarkus app! INSTALLATION To start using this extension simply add the following dependency to your quarkus app pom.xml: &lt;dependency&gt; &lt;groupId&gt;io.quarkiverse.dashbuilder&lt;/groupId&gt; &lt;artifactId&gt;quarkus-dashbuilder&lt;/artifactId&gt; &lt;version&gt;0.26.1&lt;/version&gt; &lt;/dependency&gt; Then your *.dash.(yaml|yml|json) files will be available in /dashboards web context. CONFIGURATION You can map specific dashboard files using the system property quarkus.dashbuilder.dashboards and change the dashboards web context using quarkus.dashbuilder.path: EXAMPLES We also have three examples ready for use: * Hello World: This is the simplest example where the dashboards run on the client and use an inline dataset. Remember that datasets can be any JSON Array declared directly in the dataset or coming from a URL. Learn more about it in the article . * Metrics: Datasets can also have a pre-processing for some specific formats. This is the case for . In this example we consume Quarkus Micrometer metrics.  * DataSet: It is also possible to consume data generated from the Quarkus application. In this example we build a real time chart from data coming from a Quarkus endpoint. LEARN MORE ABOUT DASHBUILDER DEVELOPMENT is a tool for building dashboards and data visualizations. It runs entirely on client and you can develop dashboards using YAML on the or using the .  Links to learn more: * * * s CONCLUSION In this article we announced the Dashbuilder Quarkus extension. We have plans for a better editor developer experience and new Dashbuilder features, so stay tuned! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Configuring Kubernetes operands through custom resources</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/23/configuring-kubernetes-operands-through-custom-resources" /><author><name>Peter Braun</name></author><id>c7d51447-a83d-46fb-8afa-2e9566d0f392</id><updated>2022-12-23T07:00:00Z</updated><published>2022-12-23T07:00:00Z</published><summary type="html">&lt;p&gt;Operators in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; often allow application developers to configure low-level aspects of their operands and secondary resources. Typically, such settings are made available on the custom resource and reconciled into the operand.&lt;/p&gt; &lt;p&gt;An example of this is the &lt;code&gt;Grafana&lt;/code&gt; custom resource of the &lt;a href="https://github.com/grafana-operator/grafana-operator"&gt;Grafana Operator&lt;/a&gt;. It exposes many configuration options that are reflected in the Grafana configuration file, but also allows you to configure properties of the Kubernetes resources in your Grafana installation. For example, you can add additional ports to your service, mount secrets into the Grafana pod, or expose additional environment variables.&lt;/p&gt; &lt;p&gt;These fields in the custom resource are reconciled and applied to the respective Kubernetes resource. This article describes some problems with this approach, describes an alternative approach that is currently under development, and weighs the strengths and weaknesses of the two approaches.&lt;/p&gt; &lt;h2&gt;The problem with exposing hand-picked configuration options&lt;/h2&gt; &lt;p&gt;The issue with hand-picking properties of Kubernetes resources, exposing them on a custom resource, and then reconciling them is that it's hard to foresee what a user might want to modify. For the Grafana Operator, we often get requests to make additional fields of underlying resources configurable through the custom resource.&lt;/p&gt; &lt;p&gt;Additionally, Kubernetes resources change over time. Granted, that happens rather slowly, but it does happen—it happened when Ingress was promoted to v1, for instance.&lt;/p&gt; &lt;p&gt;Another issue with this approach is ease of use. Someone who is already familiar with configuring, say, a route, now needs to learn how to do that through your custom resource. And often only a subset of the available options is exposed.&lt;/p&gt; &lt;h2&gt;A better approach&lt;/h2&gt; &lt;p&gt;How could the definition of custom resources be improved for both application developers and the people who release resources such as Grafana? In the upcoming version of the Grafana Operator, we have started exposing raw Kubernetes resources in the custom resource. To configure a deployment, for instance, you will have access to a &lt;code&gt;Deployment&lt;/code&gt; object complete with the official &lt;code&gt;spec&lt;/code&gt; and &lt;code&gt;metadata&lt;/code&gt;. The same goes for all other resources that are managed by the Operator, the &lt;code&gt;ServiceAccount&lt;/code&gt;, the &lt;code&gt;Route&lt;/code&gt; or &lt;code&gt;Ingress&lt;/code&gt;, and the &lt;code&gt;Service&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You don't have to learn how to configure a resource through the Grafana Operator. Instead, you can focus on the Kubernetes resources you want to configure and do the configuration in the usual manner.&lt;/p&gt; &lt;h2&gt;Difficulties&lt;/h2&gt; &lt;p&gt;Sounds easy? Not quite. There are a few obstacles to overcome.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Some Kubernetes resource definitions, such as &lt;code&gt;Deployment&lt;/code&gt;, are huge and will bloat your custom resource definition (CRD).&lt;/li&gt; &lt;li&gt;Our new way of exposing resources is not suitable for partial specification, which is what application developers usually want most.&lt;/li&gt; &lt;li&gt;We don't yet have a merge strategy to combine Operator defaults and custom overrides.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;We can tackle those issues. The following subsections cover each issue.&lt;/p&gt; &lt;h3&gt;CRD bloat&lt;/h3&gt; &lt;p&gt;To address CRD bloat, we're going to strip the descriptions. When using kubebuilder to generate the CRDs from the code, we can pass the following parameter, which cuts down the size of our CRDs by two thirds:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;crd:maxDescLen=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Partial specification&lt;/h3&gt; &lt;p&gt;Let me specify what problem we're trying to solve here. Resources such as &lt;code&gt;Deployment&lt;/code&gt; come with optional and mandatory fields. That is also true for the deployment &lt;code&gt;spec&lt;/code&gt; in our CRD. As soon as an application developer adds a non-empty &lt;code&gt;spec&lt;/code&gt; to a &lt;code&gt;Deployment&lt;/code&gt;, they are required to &lt;code&gt;spec&lt;/code&gt; out all the mandatory fields as well. This is not ideal. You might be interested in just overriding the replicas, without providing a full pod template.&lt;/p&gt; &lt;p&gt;The solution to this problem is not as simple as adding another parameter. We came across the solution while looking at Banzaicloud's &lt;a href="https://github.com/banzaicloud/operator-tools"&gt;operator-tools&lt;/a&gt; project. The idea is to provide your own definition of the &lt;code&gt;spec&lt;/code&gt; that has all the same fields but no mandatory fields.&lt;/p&gt; &lt;p&gt;For example, the original deployment &lt;code&gt;spec&lt;/code&gt; defines the pod template like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;type DeploymentSpec struct { ... Template v1.PodTemplateSpec `json:"template"` ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In our own definition, we define &lt;code&gt;v1.PodTemplateSpec&lt;/code&gt; as a pointer and add the &lt;code&gt;omitempty&lt;/code&gt; tag to prevent the serializer from adding an empty key:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;type CustomDeploymentSpec struct { ... Template *v1.PodTemplateSpec `json:"template,omitempty"` ... }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We also define our own &lt;code&gt;Deployment&lt;/code&gt; type:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;type CustomDeployment struct { ObjectMeta ObjectMeta `json:"metadata,omitempty"` Spec CustomDeploymentSpec `json:"spec,omitempty"` }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This gives us a resource with the same structure as a &lt;code&gt;Deployment&lt;/code&gt;, but all the top-level fields are optional.&lt;/p&gt; &lt;h3&gt;Merge strategy&lt;/h3&gt; &lt;p&gt;All that's left to do now is create a merge strategy to merge the overridden, custom deployment with the existing one. Our policy prefers to keep the existing fields in the original resource unless they are the defaults, and we ignore empty fields in the overridden resource.&lt;/p&gt; &lt;p&gt;Thankfully, Kubernetes's own &lt;code&gt;apimachinery&lt;/code&gt; library contains everything we need in its &lt;code&gt;strategicpatch&lt;/code&gt; package. &lt;code&gt;apimachinery&lt;/code&gt; deals with schemas and conversion. &lt;code&gt;strategicpatch&lt;/code&gt; compares two JSON representations of objects and produces a patch that can be applied to the resource definition.&lt;/p&gt; &lt;p&gt;Again, the operator-tools package contains an implementation of a merge function using &lt;code&gt;strategicpatch&lt;/code&gt;, and we use a slightly modified version of it in the Grafana Operator.&lt;/p&gt; &lt;h2&gt;Merging a custom resource field&lt;/h2&gt; &lt;p&gt;Let's see this in action. The default Grafana deployment created by the Operator looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: "1" name: grafana-a-deployment namespace: grafana spec: replicas: 1 selector: matchLabels: app: grafana-a strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We want to override the strategy to &lt;code&gt;Recreate&lt;/code&gt;, so we provide the following deployment in the Grafana custom resource &lt;code&gt;spec&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt; spec: deployment: spec: strategy: type: Recreate&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We don't need to provide any of the fields that are mandatory for deployments, only the field we are interested in overriding: &lt;code&gt;spec.strategy.type&lt;/code&gt;. This produces an updated deployment with a strategy set to &lt;code&gt;Recreate&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Evaluating the strategy of exposing resources&lt;/h2&gt; &lt;p&gt;Exposing raw Kubernetes resources is a powerful way for users to configure operands. Existing knowledge and documentation can be applied. The Operator provides everything that can be configured out of the box.&lt;/p&gt; &lt;p&gt;But disadvantages remain:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The size of the CRD still increases considerably. If you need to configure a large number of resources, this process might not be a good choice.&lt;/li&gt; &lt;li&gt;The process comes with a risk of misconfiguration. Application developers can override settings that the Operator or an operand depends on.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;But we believe that, overall, the advantages of such a flexible configuration system usually outweigh the disadvantages.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/23/configuring-kubernetes-operands-through-custom-resources" title="Configuring Kubernetes operands through custom resources"&gt;Configuring Kubernetes operands through custom resources&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Peter Braun</dc:creator><dc:date>2022-12-23T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.31.1 released!</title><link rel="alternate" href="https://blog.kie.org/2022/12/kogito-1-31-1-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/12/kogito-1-31-1-released.html</id><updated>2022-12-23T00:52:22Z</updated><content type="html">We are glad to announce that the Kogito 1.31.1 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Quarkus 2.15 integration * Upgraded Drools to version 8.31.1 to align with newly release drools-drl-quarkus-extension Quarkus extension, for more information, visit the following . For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.25.0 artifacts are available at the . A detailed changelog for 1.31.1 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">How to capture a Thread dump in Java</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-capture-a-thread-dump-in-java/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-capture-a-thread-dump-in-java/</id><updated>2022-12-22T08:05:54Z</updated><content type="html">In this tutorial, we will learn how to capture a thread dump in Java using different methods depending on the operating system and the environment in which the Java application is running. A thread dump is a snapshot of the current state of all threads in a Java application. It is useful for diagnosing problems ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Why we added restartable sequences support to glibc in RHEL 9</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/22/restartable-sequences-support-glibc-rhel-9" /><author><name>Florian Weimer</name></author><id>bd5af374-cf16-408f-97ab-8741b42eaaaf</id><updated>2022-12-22T07:00:00Z</updated><published>2022-12-22T07:00:00Z</published><summary type="html">&lt;p&gt;Restartable sequences (rseq) are a Linux feature that can maintain per-CPU data structures in userspace without relying on atomic instructions. A restartable sequence is written under the assumption that it runs from beginning to end without the kernel interrupting it and running some other code on that CPU. It can therefore access per-CPU data without further synchronization.&lt;/p&gt; &lt;p&gt;Restartable refers to the fallback mechanism that kicks in if the kernel has to reschedule execution. In this case, control is transferred to a fallback path, which can retry the execution or use a different algorithm to implement the required functionality. It turns out that this facility is sufficient to implement a variety of algorithms using per-CPU data, especially if combined with an explicit memory barrier system call.&lt;/p&gt; &lt;h2&gt;Why add rseq to Red Hat Enterprise Linux?&lt;/h2&gt; &lt;p&gt;There is not much software yet that uses restartable sequences for production purposes. Google's &lt;a href="https://github.com/google/tcmalloc"&gt;tcmalloc&lt;/a&gt; is one example. But the rseq kernel facility provides one more benefit. The kernel writes the number of the currently running CPU to a data structure shared with userspace. This is sufficient for a user-space-only (and therefore fast) implementation of the POSIX &lt;a href="https://man7.org/linux/man-pages/man3/sched_getcpu.3.html"&gt;sched_getcpu function&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why does sched_getcpu performance matter? Some database software needs a fast sched_getcpu function for optimization purposes. Traditionally, Linux provides the &lt;a href="https://man7.org/linux/man-pages/man2/getcpu.2.html"&gt;getcpu system call&lt;/a&gt; to obtain the number of the current CPU, but the system call overhead, unfortunately, defeats its use for performance optimizations. Some architectures already provide a fast &lt;a href="https://man7.org/linux/man-pages/man2/getcpu.2.html"&gt;getcpu function&lt;/a&gt; (the Linux variant of sched_getcpu) via the &lt;a href="https://man7.org/linux/man-pages/man7/vdso.7.html"&gt;vDSO acceleration mechanism&lt;/a&gt;, but it turns out that this vDSO approach is impossible to implement on AArch64. Other architectures use some hidden and otherwise unused CPU state to stash the CPU number, but there is simply no available CPU state on AArch64 that could be used for this purpose. This means that for performance parity, AArch64 really needs support for restartable sequences.&lt;/p&gt; &lt;p&gt;In Red Hat Enterprise Linux (RHEL), the sched_getcpu function is provided by the  &lt;a href="https://www.gnu.org/s/libc/"&gt;GNU C Library&lt;/a&gt; (glibc). We did not want to build a custom glibc variant just for AArch64, which is why we chose to enable rseq on all architectures. Furthermore, the rseq-based sched_getcpu function turned out to be slightly faster than the previous vDSO-based implementation.&lt;/p&gt; &lt;h2&gt;The glibc ABI impact&lt;/h2&gt; &lt;p&gt;Restartable sequences involve a memory area shared between userspace and kernel, which is updated by the kernel. The Linux kernel only supports one such area per thread, which means that all rseq-using code in the process needs to use that single area. Once glibc starts using rseq for its sched_getcpu function, direct activation of the rseq area via the rseq system call fails in applications because it has already been registered by glibc. Therefore, applications need to reuse the glibc-managed rseq area. In the glibc 2.35 upstream version, we solved this coordination problem by exposing &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Restartable-Sequences.html"&gt;three new symbols&lt;/a&gt;: __rseq_size, __rseq_flags, __rseq_offset as part of the glibc ABI (Application Binary Interface). Applications that want to use the rseq facility can use these ABI symbols to locate the rseq area and use it according to the restartable sequences protocol.&lt;/p&gt; &lt;p&gt;RHEL 9 is based on glibc 2.34, not 2.35, so it does not immediately benefit from this upstream work. Initially, we backported the rseq-enhanced sched_getcpu into Red Hat Enterprise Linux 9.0 in a disabled-by-default configuration without the new symbols. Users could activate the glibc.pthread.rseq &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Tunables.html"&gt;glibc tunable&lt;/a&gt; to enable it. This means that applications can register rseq directly with the kernel, but sched_getcpu on AArch64 remains very slow until the tunable is set manually on a per-process basis. In Red Hat Enterprise Linux 9.1, we will enable glibc's use of restartable sequences by default. But this means we also need to provide a coordination mechanism for applications to use. We could have come up with a RHEL-specific approach or a custom version of &lt;a href="https://github.com/compudj/librseq"&gt;librseq&lt;/a&gt; that applications are expected to use on RHEL. But in the end, we decided to backport the rseq-specific parts of the glibc 2.35 ABI into the glibc version that will be released with Red Hat Enterprise Linux 9.1. We have &lt;a href="https://developers.redhat.com/blog/2016/02/17/upgrading-the-gnu-c-library-within-red-hat-enterprise-linux"&gt;traditionally not augmented the glibc ABI after a release&lt;/a&gt;, but we felt that the use case was important enough to make an exception here.&lt;/p&gt; &lt;p&gt;Our RPM-based dependency management does not work on individual symbols (unlike &lt;a href="https://wiki.debian.org/UsingSymbolsFiles"&gt;Debian's symbol files&lt;/a&gt;). This means that, in general, we have to backport all symbols in the version set rather than the few we are actually interested in. In this case, we knew about the possible backport requirement when making the change upstream, so we made sure that the relevant symbol set included just the three symbols: __rseq_size, __rseq_flags, and __rseq_offset.&lt;/p&gt; &lt;p&gt;To maintain a consistent glibc ABI across the entire RHEL 9 release, a &lt;a href="https://access.redhat.com/errata/RHBA-2022:6605"&gt;RHEL 9.0 glibc update&lt;/a&gt; includes these new symbols as well. However, unlike RHEL 9.1, glibc's own use of rseq remains disabled by default. This additional backport ensures that rseq-enabled applications built against RHEL 9.1 or later can still launch on RHEL 9.0, albeit by default, without the glibc-integrated rseq facility.&lt;/p&gt; &lt;p&gt;The glibc.pthread.rseq tunable remains available in future RHEL 9 glibc versions and can be used to disable glibc's use of rseq in case there are compatibility issues. For example, we &lt;a href="https://github.com/checkpoint-restore/criu/pull/1706"&gt;worked with the CRIU authors&lt;/a&gt; to make sure that checkpointing and restoring applications continue despite rseq usage by the process, but for compatibility with checkpointing tools that lack rseq awareness, glibc's use of the feature can be disabled. Likewise, disabling glibc's use makes rseq registration available once more to early adopters of rseq that have not been ported yet to the glibc coordination mechanism (&lt;a href="https://github.com/google/tcmalloc/issues/1445%20github"&gt;such as tcmalloc&lt;/a&gt;).&lt;/p&gt; &lt;h2&gt;Restartable sequences availability&lt;/h2&gt; &lt;p&gt;Starting with Red Hat Enterprise Linux 9.1, restartable sequences are available as part of the system C library, and they are used to accelerate the sched_getcpu function that is critical to some database workloads. Maintaining compatibility with future rseq-using applications required adding new ABI symbols. A glibc update has been released for RHEL 9.0 to maintain a consistent glibc across all RHEL 9 minor releases.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/22/restartable-sequences-support-glibc-rhel-9" title="Why we added restartable sequences support to glibc in RHEL 9"&gt;Why we added restartable sequences support to glibc in RHEL 9&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Florian Weimer</dc:creator><dc:date>2022-12-22T07:00:00Z</dc:date></entry><entry><title type="html">AMA Recap</title><link rel="alternate" href="https://wildfly-security.github.io/wildfly-elytron/blog/ask-me-anything-recap/" /><author><name>Farah Juma</name></author><id>https://wildfly-security.github.io/wildfly-elytron/blog/ask-me-anything-recap/</id><updated>2022-12-22T00:00:00Z</updated><dc:creator>Farah Juma</dc:creator></entry><entry><title>Kubernetes 101 for OpenShift developers, Part 1: Components</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/12/21/kubernetes-101-openshift-developers-part-1-components" /><author><name>Bob Reselman</name></author><id>9b7b31d9-987a-4e65-bb48-ba663a7fafa3</id><updated>2022-12-21T07:00:00Z</updated><published>2022-12-21T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; has become the most prevalent &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; orchestration framework in today's enterprises. &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;, a hosted platform for Kubernetes applications, simplifies many of the development and administrative tasks Kubernetes requires, but you still need to understand Kubernetes concepts to develop applications on the platform. This article is the first in a two-part series that lays out key Kubernetes concepts for developers.&lt;/p&gt; &lt;h2&gt;Why a background in Kubernetes is important&lt;/h2&gt; &lt;p&gt;Kubernetes makes it possible to run applications composed of a large number of containers in a manner that is reliable, continuous, and failsafe. When a container running in a Kubernetes cluster fails, Kubernetes can restore the container almost immediately. Kubernetes enables teams to update applications as they're running. Also, Kubernetes provides secure access to applications at a very fine-grained level. These are just a few of the many benefits that Kubernetes provides.&lt;/p&gt; &lt;p&gt;Although Kubernetes is popular and powerful, it's not easy to use. You have to know a lot even to do a simple task. The learning curve can be daunting.&lt;/p&gt; &lt;p&gt;Fortunately, OpenShift makes working with Kubernetes a lot easier by providing a layer of abstraction that removes a good deal of Kubernetes's operational complexity from the day-to-day developer experience. This abstraction is called the &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/container-platform"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. For example, developers can use the platform's OpenShift web console to work with Kubernetes in a graphical manner. One click of a button in the web console can do the work on a Kubernetes cluster that otherwise would require fiddling with a configuration file or typing half a dozen commands into a terminal window.&lt;/p&gt; &lt;p&gt;However, OpenShift developers still need a basic understanding of what Kubernetes offers and how it works. Such knowledge is particularly important for debugging and troubleshooting the applications deployed into a Kubernetes or OpenShift cluster.&lt;/p&gt; &lt;p&gt;The purpose of this two-part series is to provide this basic understanding. This first installment provides a general overview of the Kubernetes architecture and its core components. I also describe the basic Kubernetes resources that are typically used when working with a Kubernetes cluster under the OpenShift Container Platform.&lt;/p&gt; &lt;p&gt;The second installment will explain how the OpenShift Container Platform works with Kubernetes from both a conceptual and a concrete operational perspective.&lt;/p&gt; &lt;h2&gt;Understanding the Kubernetes architecture&lt;/h2&gt; &lt;p&gt;The place to start is the basics of Kubernetes architecture, which is based on the &lt;em&gt;controller-worker&lt;/em&gt; model. As the name implies, in the controller-worker model, one computer or group of computers controls another group that does the work.&lt;/p&gt; &lt;p&gt;In Kubernetes parlance, the components and data installed in the control computer make up what is called the &lt;em&gt;control plane&lt;/em&gt; and the worker computers are called the &lt;em&gt;worker nodes&lt;/em&gt; (Figure 1.) The control plane and the worker nodes, combined, constitute a cluster.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-01_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-01_1.png?itok=35MK6Nlm" width="600" height="408" alt="The control plane performs all the control functions, delegating work to a group of workers." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The control plane performs all the control functions, delegating work to a group of workers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;One of the reasons that Kubernetes uses the controller-worker model is that the model allows a Kubernetes cluster to scale up or down on demand to meet the computational needs of the enterprise. Under Kubernetes, when a cluster reaches the limits of its computing capacity, a system administrator can automatically spin up another computer (real or virtual) and add it to the cluster in real time. Then Kubernetes automatically provisions the machine to the desired state. The cluster never needs to be taken offline. The benefits of this automation should be apparent.&lt;/p&gt; &lt;p&gt;Understanding how the control plane and worker nodes interoperate is important. I'll explain it all in a moment. But first let's take a look at how Kubernetes works in general.&lt;/p&gt; &lt;h2&gt;Understanding how Kubernetes works&lt;/h2&gt; &lt;p&gt;The fundamental building block of Kubernetes is the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; container (or its Windows version, when running under that operating system). A container isolates the process associated with it in a way that makes the process think that it's the only process on the computer. Therefore, at a logical level, the process has its own filesystem as well as special access to other system resources. Also, the process can be given its own IP address. The process is indeed isolated. No other process running on the computer has access to it.&lt;/p&gt; &lt;p&gt;Operationally, this means that you can deploy everything an application needs in a single deployment unit: the container. This deployment can include files special to the operating system, third-party libraries the application might need, and any files and other artifacts that are particular to the application.&lt;/p&gt; &lt;p&gt;For example, you can deploy an application written in Node.js in a container that has not only the application's source code and package dependencies, but the Node.js runtime. This means that the computer hosting the container doesn't need to have the Node.js runtime installed. Rather, the Node.js runtime is deployed in the container.&lt;/p&gt; &lt;p&gt;Containers bring a lot of versatility to the deployment process. You can add and remove them at a whim without having to reboot the host computer. Also, containers can be configured at a very fine grain.&lt;/p&gt; &lt;p&gt;A &lt;em&gt;pod&lt;/em&gt; is a resource in Kubernetes that logically hosts one or more containers. You can think of a pod as a logical "wrapper" for containers.&lt;/p&gt; &lt;p&gt;Running a container on a single local machine is useful, but containers really become powerful when you deploy a number of them onto a collection of computers networked together. You can distribute a number of identical containers with different IP addresses among these computers. Put all these identical containers behind a load balancer that routes network traffic to the containers evenly, and you have a fairly robust distributed application. This is the good news.&lt;/p&gt; &lt;p&gt;The bad news is that supporting such an architecture is complex and error-prone. This is where Kubernetes comes into play. Kubernetes makes it easy to deploy containerized applications from a container repository and provision the containers within the cluster automatically. And Kubernetes offers several additional features that make the technology compelling:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The Kubernetes control plane automatically figures out which of its worker nodes is best suited to host the application's containers.&lt;/li&gt; &lt;li&gt;Kubernetes creates access to the container on the cluster's internal network as part of the container deployment process.&lt;/li&gt; &lt;li&gt;Kubernetes guarantees that all the containers that are supposed to be running are always running, and replenishes a container in the event of failure.&lt;/li&gt; &lt;li&gt;Should a worker node computer fail entirely, Kubernetes recreates the containers that were running on the failing worker node onto another machine.&lt;/li&gt; &lt;li&gt;If a new worker node computer is added to the cluster, Kubernetes redistributes the containers to balance the workload.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Now that you understand what Kubernetes is and what it does, let's take a look at how it works. The place to start is the control plane.&lt;/p&gt; &lt;h2&gt;The control plane&lt;/h2&gt; &lt;p&gt;As mentioned earlier, the control plane controls activities within the cluster in general and among worker nodes in particular. The control plane can be made up of one or many computer systems, and consists of the basic components shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-02_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-02_1.png?itok=CO5o8wgM" width="600" height="474" alt="The control node contains components that communicate both with requests from the outside and with worker nodes in the cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The control node contains components that communicate both with requests from the outside and with worker nodes in the cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The components that made up the control plane are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;API server&lt;/li&gt; &lt;li&gt;Controller manager&lt;/li&gt; &lt;li&gt;Scheduler&lt;/li&gt; &lt;li&gt;Data storage&lt;/li&gt; &lt;li&gt;DNS server&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Let's discuss the details of each component in turn.&lt;/p&gt; &lt;h3&gt;API server&lt;/h3&gt; &lt;p&gt;Components in a Kubernetes cluster need to communicate with each other in order to execute tasks. The API server is the component that meets this need. It's the central access point into the control plane.&lt;/p&gt; &lt;p&gt;The API server publishes a REST interface that's used internally by components in the control plane and the worker nodes as well as services and users external to the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;You'll see how the API server plays a central role in the Kubernetes cluster in the description of the control plane and worker node components to come.&lt;/p&gt; &lt;p&gt;Developers and administrators can interact with the API server, and therefore control Kubernetes, through the &lt;a href="https://kubernetes.io/docs/reference/kubectl/"&gt;kubectl&lt;/a&gt; command. The &lt;a href="https://docs.openshift.com/container-platform/3.11/cli_reference/index.html#cli-reference-index"&gt;oc&lt;/a&gt; command performs the same role in OpenShift, for developers who prefer to use the command line instead of, or in conjunction with, the web console.&lt;/p&gt; &lt;h3&gt;Controller manager&lt;/h3&gt; &lt;p&gt;Tasks that are executed within a Kubernetes cluster are managed by a type of component called a &lt;em&gt;controller&lt;/em&gt;. For example, the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/"&gt;ReplicationController&lt;/a&gt; watches over the cluster to ensure that the pods in the cluster are working properly and replenishes pods if something goes wrong. The &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller"&gt;node controller&lt;/a&gt; watches over the various computers that make up the given Kubernetes cluster.&lt;/p&gt; &lt;p&gt;The controller manager manages all the other controllers in the cluster. This includes controllers that ship with Kubernetes by default as well as custom controllers created by developers.&lt;/p&gt; &lt;h3&gt;Scheduler&lt;/h3&gt; &lt;p&gt;The scheduler finds the best worker node in the cluster in which to install a pod. Sometimes, all the scheduler has to do is find a worker node that has enough disk space and CPU capacity to host the given pod. But a pod could be configured to have more specific requirements. For example, you can configure a pod to require that it run on a bare metal computer and not within a VM. In that case, the scheduler inspects the cluster to find a worker node that is a bare metal computer.&lt;/p&gt; &lt;p&gt;Once the most appropriate worker node has been identified, the scheduler will, as its name implies, schedule the pod to run on the node. All the interactions between the scheduler and the cluster are conducted by way of the API server.&lt;/p&gt; &lt;h3&gt;Data storage&lt;/h3&gt; &lt;p&gt;One of the reasons why Kubernetes is so effective is that it does a really good job of keeping track of the entire state of the cluster all the time. You can have a cluster that's made up of hundreds of worker nodes, with thousands of pods installed on each node, and Kubernetes knows the exact state of every node and pod in the cluster at any moment.&lt;/p&gt; &lt;p&gt;Information about the state of the cluster is stored in an &lt;a href="https://etcd.io/"&gt;etcd&lt;/a&gt; database. Any and all data that describes the state of the cluster is stored in etcd. Developers can inspect data in etcd using the &lt;a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe"&gt;describe subcommand of kubectl&lt;/a&gt;, and OpenShift developers can use the &lt;a href="https://docs.openshift.com/container-platform/4.8/cli_reference/openshift_cli/developer-cli-commands.html#oc-describe"&gt;corresponding describe subcommand of oc&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following example runs &lt;code&gt;oc describe&lt;/code&gt; to read the information from etcd about a pod named &lt;code&gt;simplehttp&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc describe pod/simplehttp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following is a snippet of the response returned from the &lt;code&gt;oc describe&lt;/code&gt; command starting from the beginning of the output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Name: simplehttp Namespace: reselbob-dev Priority: -3 Priority Class Name: sandbox-users-pods Node: ip-10-0-175-2.us-east-2.compute.internal/10.0.175.2 Start Time: Wed, 14 Dec 2022 19:04:30 +0000 Labels: role=admin Annotations: k8s.v1.cni.cncf.io/network-status: [{ "name": "openshift-sdn", "interface": "eth0", "ips": [ "10.131.2.57" ], "default": true, "dns": {} }] . . .&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;DNS server&lt;/h3&gt; &lt;p&gt;The Domain Name System (DNS) assigns easily readable names to IP addresses. For example, when you type &lt;code&gt;www.redhat.com&lt;/code&gt; into the address bar of a browser, a DNS server somewhere converts the readable name &lt;code&gt;redhat.com&lt;/code&gt; into the IP address of the Red Hat web server running on the internet. The request from the browser is then forwarded to that IP address. The scope of the network that the DNS server supports is worldwide in this case, because the internet is a worldwide, public network.&lt;/p&gt; &lt;p&gt;However, a DNS server can also be used to create and manage readable names within a private network, such as the one within a Kubernetes cluster.&lt;/p&gt; &lt;p&gt;The control plane's DNS server enables Kubernetes to use readable names instead of IP addresses to locate resources within its network. For example, the following name was generated by the DNS server within an OpenShift cluster running on my instance of the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat Developer Sandbox&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;simple-site-app.rreselma-dev.svc.cluster.local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The name is also managed by the DNS server. Thus, when I call the DNS name of the application's service from a command line within the sandbox, I'll get a response from the web server running at the internal IP address that Kubernetes assigned to my application. The following HTML output shows the response from calling the application from the command line using the &lt;code&gt;curl&lt;/code&gt; command using the application's DNS name:&lt;/p&gt; &lt;pre&gt; &lt;code class="html language-xml"&gt;&lt;!DOCTYPE html&gt; &lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Simple Site&lt;/title&gt; &lt;link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous"&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello from Simple Site!&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The important thing to remember about all of this is that the DNS server is a component of the Kubernetes control plane. The DNS server allows applications and services running within a Kubernetes cluster to interact with each other over the internal network according to the readable names that the DNS server creates and manages. (Readable names created by the Kubernetes DNS server follow a &lt;a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services"&gt;predefined naming convention&lt;/a&gt;.)&lt;/p&gt; &lt;p&gt;The naming capability provided by the DNS server becomes very important when configuring applications that interact with services internal to the Kubernetes cluster. The IP address assigned to a service might change, but the service's DNS name will not.&lt;/p&gt; &lt;h2&gt;Worker nodes&lt;/h2&gt; &lt;p&gt;A worker node is the computer or virtual machine that hosts containers running under Kubernetes. They follow the specifications configured for the Kubernetes pod representing the given containers.&lt;/p&gt; &lt;p&gt;The components hosted on a worker node are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;kubelet&lt;/li&gt; &lt;li&gt;cAdvisor&lt;/li&gt; &lt;li&gt;Kube-proxy&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following sections describe the details of each.&lt;/p&gt; &lt;h3&gt;kubelet&lt;/h3&gt; &lt;p&gt;Containers do not appear on a worker node by magic. Rather, as explained earlier, once a container specification is assigned to a pod, it's up to the scheduler to find an appropriate worker node on which to run the container. That is the purpose of kubelet: To create containers (a process called &lt;em&gt;realizing&lt;/em&gt; them) on its worker node and then monitor them.&lt;/p&gt; &lt;p&gt;Once the best worker node is found, the control plane notifies the instance of kubelet running on the identified worker node to realize a pod's container. Then, kubelet does the work of installing the container on the worker node (Figure 3) and reports back to the API server the particulars about the container installation. That information is stored in the etcd database.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-03_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-03_1.png?itok=XGcO2i1W" width="600" height="474" alt="kubelet runs on each worker node and creates pods when instructed by the API server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: kubelet runs on each worker node and creates pods when instructed by the API server. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;cAdvisor&lt;/h3&gt; &lt;p&gt;cAdvisor gathers and reports metrics about activities within the given worker node. The name is derived from a blending of the words &lt;em&gt;container&lt;/em&gt; and &lt;em&gt;advisor&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;cAdvisor is aware of all the containers running on a worker node, and collects CPU, memory, file system, and network usage statistics about the containers. This information can be viewed using reporting tools such as &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; and &lt;a href="https://grafana.com"&gt;Grafana&lt;/a&gt;. The information provided by cAdvisor is particularly useful when testing and troubleshooting an application running on Kubernetes.&lt;/p&gt; &lt;h3&gt;kube-proxy&lt;/h3&gt; &lt;p&gt;kube-proxy is a daemon that runs on each worker node in a Kubernetes cluster in order to route the network traffic received by the worker node to the appropriate container hosted on the worker node.&lt;/p&gt; &lt;p&gt;kube-proxy does this job by working with the control plane to observe how routes are defined for the service, endpoint, or &lt;a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/"&gt;EndpointSlices&lt;/a&gt; resources operating within the cluster. Using this information, kube-proxy figures out how to route an incoming request to the corresponding container on the worker node.&lt;/p&gt; &lt;p&gt;Should routing information change within the control plane, each instance of kube-proxy running in the cluster updates its respective worker node to accommodate the new routing information.&lt;/p&gt; &lt;h2&gt;Customizing Kubernetes&lt;/h2&gt; &lt;p&gt;An important feature of Kubernetes is its extendable design. Should an organization need some capability that is not available in Kubernetes by default, the organization's IT department can create a custom component to meet the need at hand. For example, a company can create a custom controller that watches for particular changes in a pod's behavior and sends a notification about the behavior change to a predefined email address.&lt;/p&gt; &lt;p&gt;Also, developers can write custom resources through what's called, in Kubernetes parlance, a &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/"&gt;custom resource definition&lt;/a&gt; (CRD). Writing a CRD is an advanced topic beyond the scope of this article. The important thing to understand about a CRD is that it extends Kubernetes beyond the capabilities provided by the basic resources.&lt;/p&gt; &lt;p&gt;That being said, it's useful to have an understanding of the basic resources that come with Kubernetes by default.&lt;/p&gt; &lt;h2&gt;Basic Kubernetes resources&lt;/h2&gt; &lt;p&gt;A Kubernetes resource, which is also known as a Kubernetes &lt;em&gt;object&lt;/em&gt; or an &lt;em&gt;API resource&lt;/em&gt;, is a logical structure within the Kubernetes ecosystem that represents a certain aspect of state or activity in a Kubernetes cluster.&lt;/p&gt; &lt;p&gt;You can think of a Kubernetes resource as a data structure that defines a "thing" that is managed by a controller within the control plane. A resource doesn't do anything on its own. Rather, it declares a data structure that allows a controller to execute behavior on the host computer using the information in the particular resource's data structure.&lt;/p&gt; &lt;p&gt;For instance, when you tell Kubernetes to create a pod (you'll read about pods in a moment), the pod doesn't magically make itself. Rather, the API server contacts a controller, such as the ReplicationController, which then creates a data structure that describes the particulars of the given pod. That data structure is stored in the control plane's etcd datastore. Then other controllers take over and do the work of realizing the pod's containers according to the information about the pod in the data store.&lt;/p&gt; &lt;p&gt;The important thing to understand is that a Kubernetes resource is not a program. Rather, it is a data structure that tells other programs how to behave.&lt;/p&gt; &lt;h3&gt;Imperative versus declarative resource realization&lt;/h3&gt; &lt;p&gt;Before I go into describing the details of the basic Kubernetes resources, it's important to understand in general how a Kubernetes resource is realized. There are two ways to realize a resource.&lt;/p&gt; &lt;p&gt;One way is called &lt;em&gt;imperative&lt;/em&gt; realization. Imperative realization takes place when you create a Kubernetes resource directly from the command line using the &lt;code&gt;kubectl&lt;/code&gt; CLI. For example, the following &lt;code&gt;kubectl&lt;/code&gt; command creates a pod that has a container running a simple HTTP server:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl run simplehttp --image=quay.io/openshift-examples/simple-http-server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The other way is called &lt;em&gt;declarative&lt;/em&gt; realization. To create a resource declaratively, you write a text file that describes the resource's configuration. Then you execute the &lt;code&gt;kubectl&lt;/code&gt; command against that file.&lt;/p&gt; &lt;p&gt;The text file is called a &lt;em&gt;manifest&lt;/em&gt; or configuration file. The text file can be created in either YAML or JSON format, although YAML is by far more prevalent. Also, the text file must be constructed according to the specification that is standard to the various Kubernetes resources.&lt;/p&gt; &lt;p&gt;The following listing shows a manifest named &lt;code&gt;server.yaml&lt;/code&gt; that creates the pod named &lt;code&gt;simplehttp&lt;/code&gt; pod, with the same result as the declarative command shown previously:&lt;/p&gt; &lt;pre&gt; &lt;code class="java language-yaml"&gt;apiVersion: v1 kind: Pod metadata: name: simplehttp labels: role: admin spec: containers: - name: httpserver image: quay.io/openshift-examples/simple-http-server ports: - name: web containerPort: 8080 protocol: TCP&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example shows the &lt;code&gt;kubectl&lt;/code&gt; command to execute against the &lt;code&gt;server.yaml&lt;/code&gt; configuration file to create the pod declaratively. The &lt;code&gt;-f&lt;/code&gt; option precedes the name of the configuration file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f server.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Typically, developers use imperative realization while experimenting with containers during their development activities. But when it comes to production work, particularly around releases that require some sort of &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous integration/continuous deployment&lt;/a&gt; (CI/CD) automation, declarative realization is preferred.&lt;/p&gt; &lt;p&gt;Now that I've covered declarative versus imperative realization, let's move on and take a look at the resources typically used when creating an application that runs on Kubernetes.&lt;/p&gt; &lt;h3&gt;Pod&lt;/h3&gt; &lt;p&gt;As mentioned earlier, a &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;pod&lt;/a&gt; is a logical wrapper for one or more containers hosted within a Kubernetes cluster. A pod defines the container image (or multiple images) along with the associated image repository to use to realize the corresponding containers. The pod can define the environmental variables to be created in the given container's process. The pod can also define startup behavior in the form of an &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"&gt;initContainer&lt;/a&gt;, as well as the type of machines the containers require.&lt;/p&gt; &lt;p&gt;A pod has an IP address that is unique to the cluster. This IP address will be shared by the containers assigned to the pod. However, each container in a pod can have a port number unique to the container.&lt;/p&gt; &lt;p&gt;I've mentioned just a few of the configuration settings available when defining a pod.&lt;/p&gt; &lt;p&gt;The implementation of a &lt;em&gt;sidecar pattern&lt;/em&gt; is one common use case for a pod with multiple containers. The sidecar pattern is, as the name implies, an architectural design pattern in which one container uses services available in another container. The second container acts as a "sidecar" to the first. The benefit of putting two containers in the same pod to implement the sidecar pattern is that the containers are installed on the same machine and share the same IP address, requiring minimal network overhead.&lt;/p&gt; &lt;h3&gt;Deployment&lt;/h3&gt; &lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;deployment&lt;/a&gt; is a Kubernetes resource that gathers a set of identical pods and guarantees that they run continuously in a Kubernetes cluster, even if individual pods fail. Although a pod can be created as a standalone resource, pods are typically created by way of a deployment.&lt;/p&gt; &lt;p&gt;In addition to ensuring the continuous operation of all its pods, a deployment offers another benefit of scaling on demand. This means that pods can be added or removed from a deployment at runtime without affecting the cluster's operation.&lt;/p&gt; &lt;p&gt;In short, deployments provide resilience against failure as well as the ability to scale pods up or down to meet an application's operational demands of the moment.&lt;/p&gt; &lt;h3&gt;Service&lt;/h3&gt; &lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;service&lt;/a&gt; is a Kubernetes resource that represents a pod's logic on the internal Kubernetes network. Typically, users and other services interact with pods by way of a service resource. It's very rare to interact with a pod directly.&lt;/p&gt; &lt;p&gt;Kubernetes associates a service to a set of pods using labels. A label is assigned to a pod either imperatively at the command line or declaratively within its manifest file. Then, a parameter within the service's manifest file describes which pods a service should use according to labels defined in those pods (Figure 4).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-04_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-04_1.png?itok=2h9vbQKG" width="365" height="310" alt="Services use pods based on labels defined in the configuration file or on the command line." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Services use pods based on labels defined in the configuration file or on the command line. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Namespace&lt;/h3&gt; &lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"&gt;namespace&lt;/a&gt; is a Kubernetes resource that enables Kubernetes resources within a given cluster to be isolated and given a readable name. Once a Kubernetes resource is assigned to a namespace, it can see and be seen only by other resources within that namespace (Figure 5).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-05_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-05_1.png?itok=6zSVd8Fa" width="601" height="176" alt="Each namespace contains resources that can be reached only through the name assigned to the namespace." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Each namespace contains resources that can be reached only through the name assigned to the namespace. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Secret&lt;/h3&gt; &lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;secret&lt;/a&gt; is a Kubernetes resource that stores encrypted information of interest to other resources in the given Kubernetes cluster. Typically, secrets store sensitive information such as access credentials to a database. At the enterprise level, secrets are usually managed by system administrators and security personnel. These administrators give developers access to a secret according to its resource name. The developers then configure their applications to use information in the secret at run time.&lt;/p&gt; &lt;p&gt;The benefit that a secret provides is that developers can use the information within a secret without actually knowing the exact content of that information.&lt;/p&gt; &lt;h3&gt;ConfigMap&lt;/h3&gt; &lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/"&gt;ConfigMap&lt;/a&gt; is a Kubernetes resource that stores and exposes configuration information of interest to other resources in a given Kubernetes cluster. Unlike a secret, which is encrypted, information in a ConfigMap is stored as clear text key-value pairs.&lt;/p&gt; &lt;h3&gt;Persistent volume&lt;/h3&gt; &lt;p&gt;A &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/"&gt;persistent volume&lt;/a&gt; is a Kubernetes resource that represents disk storage in a Kubernetes cluster. The benefit of a persistent volume is that it provides a way for containers to store information independently of a container's file system. By default, a container stores information in its own file system, and that information is deleted when the container is deleted. When a container stores information in a persistent volume, that information remains available even when the container is deleted.&lt;/p&gt; &lt;h3&gt;Persistent volume claim&lt;/h3&gt; &lt;p&gt;A persistent volume claim is a Kubernetes resource that allots disk storage in a Kubernetes cluster.&lt;/p&gt; &lt;p&gt;You can think of a persistent volume as a hotel, and a persistent volume claim as a room in the hotel. Users don't reserve the hotel; rather, they reserve an individual room. As with a hotel, in Kubernetes developers don't use the persistent volume directly. Rather, they ask for a portion of the persistent volume, in the form of a persistent volume claim (Figure 6). Once granted, the storage in the persistent volume claim can store a container's data.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-06_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure-06_0.png?itok=zp1tG9B_" width="515" height="349" alt="Pods use the storage granted by persistent volume claims." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Pods use the storage granted by persistent volume claims. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Kubernetes components and resources enable great flexibility and scaling&lt;/h2&gt; &lt;p&gt;This article described what Kubernetes is and how it works. You learned how Kubernetes implements the controller-worker model by way of the control plane and worker nodes. You learned the basic components of the control plane: The API server, the controller manager, the scheduler, etcd data storage, and DNS server. In addition, you learned about the components hosted in every worker node: The kubelet, cAdvisor, and kube-proxy.&lt;/p&gt; &lt;p&gt;Finally, you learned about imperative versus declarative realization as well as the basic Kubernetes resources: Pods, deployments, services, namespaces, secrets, ConfigMaps, persistent volumes, and persistent volume claims.&lt;/p&gt; &lt;p&gt;The next article in this series will describe how the OpenShift Container Platform adapts and enhances Kubernetes to make the developer experience easier and more efficient.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/12/21/kubernetes-101-openshift-developers-part-1-components" title="Kubernetes 101 for OpenShift developers, Part 1: Components"&gt;Kubernetes 101 for OpenShift developers, Part 1: Components&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-12-21T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.15.1.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-15-1-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-15-1-final-released/</id><updated>2022-12-21T00:00:00Z</updated><published>2022-12-21T00:00:00Z</published><summary type="html">It is the end of the year (again) and here comes our last release of the year, 2.15.1.Final. We had 9 minor releases this year (starting with 2.7 in February) and so many micros. We made tremendous progress on Quarkus itself and the activity in the ecosystem of extensions has...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-12-21T00:00:00Z</dc:date></entry><entry><title type="html">Getting started with jBPM Script Tasks</title><link rel="alternate" href="http://www.mastertheboss.com/bpm/jbpm6/getting-started-with-jbpm-script-tasks/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/bpm/jbpm6/getting-started-with-jbpm-script-tasks/</id><updated>2022-12-20T08:33:18Z</updated><content type="html">Script tasks allow you to execute custom scripts or code snippets as part of a jBPM process flow. This can be useful for performing complex calculations, accessing external data sources, or integrating with other systems. In this tutorial, we will look at how to create and use script tasks in jBPM. Prerequisites To follow this ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
